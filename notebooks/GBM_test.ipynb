{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "705d71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation --> work with log-returns\n",
    "# Parameters estimation (μ: Drift and σ: Volatility)\n",
    "# Montecarlo Simulation for SPY testing a lot of trajectories (using Ito's correction)\n",
    "# Evaluation (RMSE and MAE)\n",
    "# Prediction (30 day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63aa711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79a4af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configurations\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "procesed_data_path = os.getenv('PROCESSED_DATA_PATH')  \n",
    "out_diverse_path = os.getenv('OUT_DIVERSE_PATH')     \n",
    "eval_pred_gbm_path = os.getenv('OUT_EVAL_PRED_12_GBM')  \n",
    "\n",
    "procesed_data_path = Path(procesed_data_path)\n",
    "out_diverse_path = Path(out_diverse_path)\n",
    "eval_pred_gbm_path = Path(eval_pred_gbm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf62d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading log returns dataset...\n",
      "    - Train (90%): 4904 trading days\n",
      "    - Test (10%):   545 trading days\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Preparation for SPY log-returns\n",
    "\n",
    "def data_preparation_lgreturns():\n",
    "\n",
    "    print(\"Loading SPY log-returns dataset...\")\n",
    "\n",
    "    df = pd.read_csv(procesed_data_path / 'spy_daily_log_returns.csv', index_col = 0, parse_dates = True)\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Turn into businnes days (log-returns column)\n",
    "\n",
    "    df = df['SPY_Returns'].asfreq('B').ffill().bfill()\n",
    "\n",
    "    # Sort the dataframe by date (checkup)\n",
    "\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Define cuts (90% for train and 10% for evaluation)\n",
    "\n",
    "    n = len(df)\n",
    "    train_len = int(n * 0.9)\n",
    "\n",
    "    train_lgreturns = df.iloc[:train_len]\n",
    "    test_lgreturns = df.iloc[train_len:]\n",
    "\n",
    "    print(f\"    - Train log-returns (90%): {train_lgreturns.shape[0]} trading days\")\n",
    "    print(f\"    - Test log-returns (10%):   {test_lgreturns.shape[0]} trading days\")\n",
    "\n",
    "    return train_lgreturns, test_lgreturns\n",
    "\n",
    "\n",
    "train_lgreturns, test_lgreturns = data_preparation_lgreturns() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba2224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annualized Drift or Average Growth Rate:  0.111\n",
      "Annualized Volatility:  0.192\n",
      "KS statistic: 0.10765237577808362\n",
      "p-value: 6.024046471842746e-50\n",
      "The null hypothesis is rejected (the data does not follow a normal distribution)\n",
      "------------------------------------------------------------\n",
      "Drift - Average Growth Rate (μ):  0.000442\n",
      "Volatility - Uncertainty (σ):  0.012118\n"
     ]
    }
   ],
   "source": [
    "# 2. Parameter Estimations and KS Test according to data\n",
    "\n",
    "# Parameter Estimation with Train set\n",
    "# Display annualized parameter values (for informational purposes only)\n",
    "# Kolmogorov–Smirnov Test (KS Test) ---> to assess whether the log returns of the S&P 500 follow a normal distribution, as assumed by the GBM\n",
    "\n",
    "# H₀ (null hypothesis): log-normal returns follow a Normal(μ, σ)\n",
    "# H₁: returns do not follow a normal distribution\n",
    "# If H₀ get rejected, the GBM does not accurately describe the actual data\n",
    "\n",
    "\n",
    "def params_calibration(train):\n",
    "\n",
    "    # Historical Drift Estimation\n",
    "\n",
    "    mu = np.mean(train)\n",
    "    annualized_mu = mu * 252 \n",
    "\n",
    "    print(\"Annualized Drift or Average Growth Rate: \", round(annualized_mu, 3))  \n",
    "\n",
    "    # Historical Volatility Estimation\n",
    "\n",
    "    sigma = np.std(train)\n",
    "    annualized_sigma = sigma * np.sqrt(252)\n",
    "\n",
    "    print(\"Annualized Volatility: \", round(annualized_sigma, 3))\n",
    "\n",
    "\n",
    "    # Kolmogorov–Smirnov Test (KS Test) \n",
    "\n",
    "    # Scale\n",
    "\n",
    "    z = (train - mu) / sigma\n",
    "\n",
    "    ks_stat, p_value = stats.kstest(z, 'norm')\n",
    "\n",
    "    print(\"KS statistic:\", ks_stat)\n",
    "    print(\"p-value:\", p_value)\n",
    "\n",
    "    alpha = 0.05\n",
    "\n",
    "    if p_value >= alpha:\n",
    "\n",
    "        print(\"The null hypothesis is not rejected (the data follow a normal distribution)\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"The null hypothesis is rejected (the data does not follow a normal distribution)\")\n",
    "\n",
    "\n",
    "    print(\"--\" * 30)\n",
    "    print(\"Drift - Average Growth Rate (μ): \", round(mu, 6))\n",
    "    print(\"Volatility - Uncertainty (σ): \", round(sigma, 6))\n",
    "\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "mu, sigma = params_calibration(train_lgreturns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e60f433",
   "metadata": {},
   "source": [
    "The Kolmogorov–Smirnov Test applied to the log-returns of the S&P 500 rejects the normality assumption (KS = 0.108, p < 6.02e^-50). This demonstrates that the assumption of Gaussian returns in the Geometric Brownian Motion model is not met. The model is so simple to \"explain it\". However, the simulations will be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee727ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data preparation for SPY Close Price\n",
    "\n",
    "def data_preparation_price():\n",
    "\n",
    "    print(\"Loading SPY close price dataset...\")\n",
    "\n",
    "    df = pd.read_csv(procesed_data_path / 'spy_daily_close.csv', index_col = 0, parse_dates = True)\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Turn into businnes days (Close Price column)\n",
    "\n",
    "    df = df['Close'].asfreq('B').ffill().bfill()\n",
    "\n",
    "    # Sort the dataframe by date (checkup)\n",
    "\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Define cuts (90% for train and 10% for evaluation)\n",
    "\n",
    "    n = len(df)\n",
    "    train_len = int(n * 0.9)\n",
    "\n",
    "    train_price = df.iloc[:train_len]\n",
    "    test_price = df.iloc[train_len:]\n",
    "\n",
    "    print(f\"    - Train close price (90%): {train_price.shape[0]} trading days\")\n",
    "    print(f\"    - Test close price (10%):   {test_price.shape[0]} trading days\")\n",
    "\n",
    "    return train_price, test_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca467b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. GBM Montecarlo Simulation\n",
    "\n",
    "# Generates n_sims price trajectories for n_days and returns a matrix of shape (n_days, n_sims)\n",
    "\n",
    "def simulation_gbm_paths(S0, mu, sigma, n_days, n_sims = 10000):\n",
    "\n",
    "    # Dailt Step (t + 1)\n",
    "\n",
    "    dt = 1\n",
    "\n",
    "    # Initialize matrix\n",
    "\n",
    "    paths = np.zeros((n_days, n_sims))\n",
    "\n",
    "    paths[0] = S0\n",
    "\n",
    "    # Ito's Correction for Drift for the geometric simulation ---> drift = (mu - 0.5 * sigma^2) * dt\n",
    "\n",
    "    drift = (mu - 0.5 * (sigma ** 2)) * dt\n",
    "    volatility = sigma * np.sqrt(dt)\n",
    "\n",
    "    # Random Shocks (Brownian Motion) with Normal Distribution --> Z ~ N(0, 1)\n",
    "\n",
    "    Z = np.random.normal(0, 1, (n_days - 1, n_sims))\n",
    "\n",
    "    # Vectorized path calculation --> S_t = S_{t-1} * exp(drift + vol * Z)\n",
    "\n",
    "    for t in range(1, n_days):\n",
    "\n",
    "        paths[t] = paths[t-1] * np.exp(drift + volatility * Z[t - 1])\n",
    "\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d45123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluation using the test dataset (close price)\n",
    "\n",
    "def evaluate_gbm(test_data, S0, mu, sigma, n_sims = 10000):\n",
    "\n",
    "    print(\"Evaluating the simulation in contrast with test dataset, it contains: \", len(test_data), \" days...\")\n",
    "\n",
    "    n_days = len(test_data)\n",
    "\n",
    "    # Execute simulation using the previous function (simulation_gbm_paths)\n",
    "\n",
    "    paths = simulation_gbm_paths(S0, mu, sigma, n_days, n_sims)\n",
    "\n",
    "    # Mean and Median calculation\n",
    "\n",
    "    mean_path = np.mean(paths, axis = 1)\n",
    "    median_path = np.median(paths, axis = 1)\n",
    "\n",
    "    # Confidence Intervals (5% and 95%)\n",
    "\n",
    "    lower_bound = np.percentile(paths, 5, axis = 1)\n",
    "    upper_bound = np.percentile(paths, 95, axis = 1)\n",
    "\n",
    "    # Metrics Calculation (RMSE and MAE in contrast with mean and median measures)\n",
    "\n",
    "    rmse_mean = np.sqrt(mean_squared_error(test_data.values, mean_path))\n",
    "    rmse_median = np.sqrt(mean_squared_error(test_data.values, median_path))\n",
    "\n",
    "    mae_mean = mean_absolute_error(test_data.values, mean_path)\n",
    "    mae_median = mean_absolute_error(test_data.values, median_path)\n",
    "\n",
    "    print(f\"   - RMSE (Mean Path):   $ {rmse_mean:.2f}\")\n",
    "    print(f\"   - RMSE (Median Path): $ {rmse_median:.2f}\")\n",
    "    print(f\"   - MAE (Mean Path):   $ {mae_mean:.2f}\")\n",
    "    print(f\"   - MAE (Median Path): $ {mae_median:.2f}\")\n",
    "\n",
    "\n",
    "    # Visualization # 1\n",
    "\n",
    "    plt.figure(figsize = (30, 30))\n",
    "\n",
    "    plt.semilogy(paths, color = 'gray', alpha = 0.3)\n",
    "    plt.semilogy(mean_path, color = 'red', label = 'Mean Path GBM', linewidth = 3)\n",
    "    plt.semilogy(median_path, color = 'blue', label = 'Median Path GBM', linewidth = 3)\n",
    "\n",
    "    plt.title('GBM SPY Close Price SImulation', fontsize = 25)\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Price')\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize = 18)\n",
    "    plt.savefig(out_diverse_path / '12_gbm_trajectories.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Visualization #1 Saved\")\n",
    "\n",
    "\n",
    "    # Visualization # 2\n",
    "\n",
    "    plt.figure(figsize = (30, 30))\n",
    "\n",
    "    plt.semilogy(test_data.index, test_data.values, color = 'black', label = 'Real SPY', linewidth = 2)\n",
    "    plt.semilogy(test_data.index, mean_path, color = 'royalblue', linestyle = '--', label = 'GBM Mean Path', alpha = 0.7)\n",
    "    plt.semilogy(test_data.index, median_path, color = 'orangered', linestyle = '--', label = 'GBM Median Path', alpha = 0.7)\n",
    "\n",
    "    # Probability Cone\n",
    "\n",
    "    plt.fill_between(test_data.index, lower_bound, upper_bound, color = 'silver', alpha = 0.3, label = '90% Confidence Interval')\n",
    "\n",
    "    plt.title(f'GBM Validation: Simulation vs Real Prices (n = {n_sims})')\n",
    "    plt.ylabel('SPY Close Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha = 0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_diverse_path / '12_gbm_test_validation.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Visualization #2 Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Future Prediction (30 day horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Pipeline Geometric Brownian Motion...\")\n",
    "\n",
    "    # 1. Data preparation for SPY log-returns\n",
    "\n",
    "    train_lgreturns, test_lgreturns = data_preparation_lgreturns() \n",
    "\n",
    "    # 2. Parameter Estimations and KS Test according to log returns data\n",
    "\n",
    "    mu, sigma = params_calibration(train_lgreturns)\n",
    "\n",
    "    # 3. Data preparation for SPY Close Price\n",
    "\n",
    "    train_price, test_price = data_preparation_price()\n",
    "\n",
    "    # 4. GBM Montecarlo Simulation\n",
    "\n",
    "    #  The function will be invoked in the evaluation module\n",
    "\n",
    "    #  Important to get the last price (S0 parameter --> Last train price)\n",
    "\n",
    "    last_train_price = train_price[-1]\n",
    "\n",
    "    # 5. GBM Evaluation \n",
    "\n",
    "    evaluate_gbm(test_price, last_train_price, mu, sigma, n_sims = 10000)\n",
    "\n",
    "    # 6. Future Prediction\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
