# Feature Selection for Training Set Relevance Analysis

# Methods implemented:

#   - Random Forest Importance (MDI)
#   - Permutation Importance
#   - Mutual Information
#
# This script loads the prepared training dataset, generates a forward return target, computes 
# feature relevance through multiple methods, and exports a unified ranking + visual report.


# Imports

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from pathlib import Path
from dotenv import load_dotenv

from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import mutual_info_regression
from sklearn.inspection import permutation_importance


# Paths

load_dotenv()

prepared_data_path = os.getenv("PREPARED_DATA_PATH")
output_path = os.getenv("OUT_FEATURES_PATH")

if prepared_data_path:

    prepared_data_path = Path(prepared_data_path)

if output_path:

    output_path = Path(output_path)
    os.makedirs(output_path, exist_ok = True)



# Function: Load only the training dataset (prevents data leakage)

def load_train_data():

    print("Loading training dataset...")

    df = pd.read_csv(prepared_data_path / "train_dataset.csv", index_col = 0, parse_dates = True)

    print(f"File shape: {df.shape}")

    return df


# Function: Prepare analysis and generates a forward return target and separates X / y

def prepare_analysis(df, target_column = 'Close_SPY', horizon = 10):

    data = df.copy()

    # 1. Create forward return as analytic target (n-day horizon)

    data['target_return'] = data[target_column].pct_change(horizon).shift(-horizon)

    # 2. Clean invalid values generated by pct_change / shift

    data.replace([np.inf, -np.inf], np.nan, inplace = True)

    # 3. Remove rows with NaN values due to shifting

    data.dropna(subset = ['target_return'], inplace = True)

    # 4. Separate features (X) and target (y)

    drop_cols = [target_column, 'target_return']
    cols_to_drop = [col for col in drop_cols if col in data.columns]

    X = data.drop(columns=cols_to_drop)
    y = data['target_return']

    print(f"Analyzed features: {X.shape[1]}")
    print(f"Valid observations: {X.shape[0]}")

    return X, y


# Random Forest Analysis (MDI + Permutation Importance)

def random_forest_analysis(X, y):

    print("Random Forest Importance Analysis (MDI + Permutation)...")

    rf_model = RandomForestRegressor(
        n_estimators = 100,
        max_depth = 8,
        random_state = 42,
        n_jobs = -1
    )

    rf_model.fit(X, y)

    # Native Feature Importance - MDI (Mean Decrease Impurity)

    importances_mdi = pd.Series(rf_model.feature_importances_, index = X.columns)

    # Permutation Importance 

    print("Computing Permutation Importance...")

    perm_results = permutation_importance(
        rf_model, X, y,
        n_repeats = 10,
        random_state = 42,
        n_jobs = -1
    )

    importances_perm = pd.Series(perm_results.importances_mean, index = X.columns)

    return importances_mdi, importances_perm, rf_model



# Mutual Information Analysis (Captures non-linear relationships between X and y)


def mutual_information_analysis(X, y):

    print("Mutual Information Analysis...")

    mi_scores = mutual_info_regression(X, y, discrete_features = False, random_state = 42)

    mi_series = pd.Series(mi_scores, index = X.columns)

    return mi_series



# Plotting and Saving Combined Analysis
#   - Consensus ranking CSV
#   - Feature comparison chart (top 20)
#   - Correlation matrix (top 10 features)


def plot_save_analysis(X, mdi_series, perm_series, mi_series):

    print("Generating unified importance report...")

    # Combine all metrics into a single DataFrame

    results = pd.DataFrame({
        'RF_MDI':          mdi_series,
        'RF_Permutation':  perm_series,
        'Mutual_Info':     mi_series
    })

    # Normalization for visual scale (Min-Max)

    for col in results.columns:

        if results[col].max() > results[col].min():  

            results[f"{col}_Norm"] = (
                results[col] - results[col].min()
            ) / (results[col].max() - results[col].min())

        else:

            results[f"{col}_Norm"] = 0

    # Consensus Score: average of normalized metrics

    results['Score_Avg'] = (
        results['RF_MDI_Norm'] +
        results['RF_Permutation_Norm'] +
        results['Mutual_Info_Norm']
    ) / 3

    # Sort by consensus score

    results = results.sort_values(by = 'Score_Avg', ascending = False)

    # Save CSV ranking

    csv_path = output_path / "feature_ranking_consensus.csv"
    results.to_csv(csv_path)

    print(f"CSV saved to: {csv_path}")

    # Figure 1: Top 25 Feature Consensus

    print("Creating feature comparison chart (Top 25)")

    plt.figure(figsize = (24, 22))

    top_25 = results.head(25)
    y_pos = np.arange(len(top_25))
    h = 0.25

    plt.barh(y_pos + h, top_25['RF_MDI_Norm'], h, label = 'RF MDI',        color = "#050f7b", alpha = 0.7)
    plt.barh(y_pos, top_25['RF_Permutation_Norm'], h, label = 'Permutation',   color = "#045B38CF", alpha = 0.9)
    plt.barh(y_pos - h, top_25['Mutual_Info_Norm'], h, label = 'Mutual Info',   color = "#861d08", alpha = 0.8)

    plt.yticks(y_pos, top_25.index)
    plt.xlabel("Normalized Importance")
    plt.title("Feature Importance Consensus: MDI vs Permutation vs MI")
    plt.legend()
    plt.gca().invert_yaxis()
    plt.grid(axis = 'x', alpha = 0.3)
    plt.tight_layout()

    plot_path = output_path / "feature_importance_comparison.png"
    plt.savefig(plot_path)
    plt.close()

    print(f"Chart saved to: {plot_path}")

    # Figure 2: Correlation Matrix of Top 15 Features

    print("Generating Top-15 Correlation Matrix...")

    top10 = results.index[:15].tolist()
    X_top = X[top10]

    corr_matrix = X_top.corr()

    plt.figure(figsize = (24, 22))

    mask = np.triu(np.ones_like(corr_matrix, dtype = bool))

    sns.heatmap(
        corr_matrix,
        mask = mask,
        annot = True,
        fmt = ".2f",
        cmap = "inferno",
        center = 0,
        vmin = -1, vmax = 1,
        square = True,
        linewidths = 0.5,
        cbar_kws = {"shrink": 0.5}
    )

    plt.title("Correlation Matrix of Top 15 Features")
    plt.tight_layout()

    corr_path = output_path / "top15_features_correlation.png"

    plt.savefig(corr_path)
    plt.close()

    print(f"Correlation matrix saved to: {corr_path}")

    return results



# Main: Execution

if __name__ == '__main__':

    df_train = load_train_data()

    if df_train is not None:

        # Prepare inputs

        X, y = prepare_analysis(df_train, target_column = 'Close_SPY', horizon = 10)

        # Random Forest Metrics

        rf_mdi, rf_perm, rf_model = random_forest_analysis(X, y)

        # Mutual Information

        mi_scores = mutual_information_analysis(X, y)

        # Full Report Generation

        ranking = plot_save_analysis(X, rf_mdi, rf_perm, mi_scores)

        print("\n=== TOP 10 FEATURES (Consensus Score) ===")
        print(ranking.head(10)[['Score_Avg', 'RF_Permutation_Norm', 'Mutual_Info_Norm']])